---
title: "Mixed Effects Modelling in R"
subtitle: "University of Tehran"
author: "Dr Ehsan Solaimani"
date: "19 and 21 Azar"
output:
  xaringan::moon_reader:
    css:
      - xaringan-themer.css
      - custom.css
    nature:
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: true
      beforeInit: "macros.js" 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse) #for data manipulation
library(lme4) #for mixed effects models
#library(lmerTest) #for p-values in mixed effects models
library(effectsize) #for effect size
library(sjPlot) #for plots
library(flextable) #create tables
library(kableExtra)
options(warn = -1)
```

## What this workshop will cover
- Basics of general linear models (GLM)
- Fixed vs. random effects
- Contrast coding to deal with categorical variables
- How to run mixed effects models  and interpret the output
---
## Generalised Linear Models

- Helps examine the relationship between two or more variables
- Dependent variable is the variable that is observed, measured, or recorded in an experiment and is affected by the independent variable.
- Independent variables are all the other variables that are observed, manipulated, or controlled by the experimenter and are believed to have an effect on the dependent variable.

---

## GLM (continued)
- Assumes a linear relationship between the DV and IVs
  - <font size="+0.001"> $y= b_{1}x+b_{0}$ </font>
  - <font size="+0.001"> $y:Dependent$ </font>
  - <font size="+0.001"> $x:Independent$ </font>
  - <font size="+0.001"> $b_{0}:intercept$ </font>
  - <font size="+0.001"> $b_{1}:slope$ </font>

- We know x and y, but we want to estimate b0 and b1
---

## GLM (continued)

As an example, below the slope = 2 and intercept = 200

```{r casdrs, echo = F, warning=F, fig.width = 8, fig.height = 3}
set.seed(123)
data <- data.frame(x = 0:150)
data$y <- 2 * data$x + 200 +rnorm(80,0,35)   

#create plot
ggplot(data, aes(x = x, y = y)) +
  geom_smooth(formula = y ~ x,method = "lm") +
  geom_point() +
  geom_text(aes(x = 18, y = 400), label = "y = 2x + 200", color = "black", size = 5) +
  geom_text(aes(x = 18, y = 500), label = "y = b1x + b0", color = "black", size = 5) +
  theme_bw() +
  theme(axis.text = element_text(size = 12),
        axis.title = element_text(size = 14))

```

---

## GLM (continued)

The intercept $b_{0}$ is the point where the GLM intersects the y-axis. It signifies the baseline prediction when the  independent variable is zero.

```{r casdvsrs, echo = F, warning=F, fig.width = 8, fig.height = 3}
#create plot
ggplot(data, aes(x = x, y = y)) +
  geom_smooth(formula = y ~ x,method = "lm") +
  geom_point() +
  geom_text(aes(x = 18, y = 400), label = "y = 2x + 200", color = "black", size = 5) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "black") +
  geom_hline(yintercept = 200, linetype = "dashed", color = "black") +  # Add horizontal line at y = 200
  theme_bw() +
  theme(axis.text = element_text(size = 12),
        axis.title = element_text(size = 14))
```

---

## GLM (continued)

The slope $b_{1}$ indicates how steep the line is, i.e., the larger the slope, the steeper the line. It shows how much $y$ will change, given one unit of increase in $x$

```{r cassddvsrs, echo = F, warning=F, fig.width = 8, fig.height = 3}
#create plot
ggplot(data, aes(x = x, y = y)) +
  geom_smooth(formula = y ~ x,method = "lm") +
  geom_point() +
  geom_text(aes(x = 18, y = 400), label = "y = 2x + 200", color = "black", size = 5) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "black") +
  geom_hline(yintercept = 200, linetype = "dashed", color = "black") +  # Add horizontal line at y = 200
  theme_bw() +
  theme(axis.text = element_text(size = 12),
        axis.title = element_text(size = 14))
```


---
## Same slope, different intercepts

```{r cdfdfasdrs, echo = F, warning=F, fig.width = 10, fig.height = 6}
data <- data.frame(x = rep(0:8, times = 3),
                   intercept = rep(c(0, 50, 100), each = 9))

# Calculate corresponding y values using the equation y = 2x + intercept
data$y <- 40 * data$x + data$intercept

# Create the plot
ggplot(data, aes(x = x, y = y, color = as.factor(intercept))) +
    geom_line(size = 2) +   # Thicker lines
    geom_text(aes(x = 4, y = 100, label = "y = 2x + 100"), color = "blue", size = 5) +  # Add equation text
      geom_text(aes(x = 4, y = 80, label = "y = 2x + 50"), color = "green", size = 5) +  # Add equation text
        geom_text(aes(x = 4, y = 60, label = "y = 2x"), color = "red", size = 5) +  # Add equation text
    theme_bw() +                      # Optional: Apply a minimal theme
    theme(axis.text = element_text(size = 12), # Optional: Adjust text size
          axis.title = element_text(size = 14),
          legend.position = "none") +  # Optional: Adjust text size
    scale_color_discrete(name = "Intercept")  # Optional: Legend title
```

---
## Different slopes, same intercept

```{r cdfrfdfasdrs, echo = F, warning=F, fig.width = 10, fig.height = 6}
# Create a data frame with x values
data <- data.frame(x = rep(0:8, times = 3),
                   slope = rep(c(-100, +100, 200), each = 9),  # Different slopes
                   intercept = rep(c(200, 200, 200), each = 9))  # Same intercept

# Calculate corresponding y values using the equation y = slope*x + intercept
data$y <- data$slope * data$x + data$intercept

# Create the plot
ggplot(data, aes(x = x, y = y, color = as.factor(slope))) +
  geom_line(size = 2) +   # Thicker lines
geom_text(aes(x = 4, y = 400, label = "y = 200x + 200"), color = "blue", size = 5) +  # Add equation text
      geom_text(aes(x = 4, y = 300, label = "y = 100x + 200"), color = "green", size = 5) +  # Add equation text
        geom_text(aes(x = 4, y = 200, label = "y = -100x + 200"), color = "red", size = 5) +  # Add equation text  
  theme_bw() +                      # Optional: Apply a minimal theme
  theme(axis.text = element_text(size = 12), # Optional: Adjust text size
        axis.title = element_text(size = 14),
        legend.position = "none") +  # Optional: Adjust text size
  scale_color_discrete(name = "Slope")  # Optional: Legend title 

```

---
## Example 1

Suppose we want to examine the relationship between lexical access and L2 proficiency. An intuitive hypothesis is that with more proficiency, lexical access is faster. We conduct a lexical decision task to investigate this.

**Presentation**: A word or a non-word (a random string of letters) is presented to the participant on a screen.

**Decision**: The participant must quickly decide whether the presented string is a real word or not.

**Response**: The participant responds by pressing a key, often one key for "word" and another for "non-word", while
their reaction times are recorded to reflect difficulty in lexical access.
---

## Example 1 (continued)

First, we load the data:

```{r cdfrdsaffdfasdrs, echo = T, warning=F}
design_lexc <-read.csv("design_lexc.csv")
```

```{r cdfrddvsdfdaffdfasdrs, echo = F, warning=F}
design_lexc %>% slice(1:6)%>% kbl(align = "l") %>% kable_classic(full_width = F, html_font = "Cambria")
```
---
## Example 1 (continued)

We fit a GLM to predict reaction times (RT) as a function of proficiency (prof).

```{r cdefrdddvsaffdfasdrs, echo = T,  eval=FALSE, warning=F}
model00<-lm(RT~prof, design_lexc) #fit model
summary(model00) #see output (next page)
```

---

## Example 1 (continued)

```{r cdfrddvsafssfdfasdrs, echo = F, warning=F}
model00<-lm(RT~prof, design_lexc) #fit model
summary(model00)
```

---

```{r cdfrddsdvsadcffdfasdrs, echo = T, warning=F}
summary(model00)
```

- *call* reminds you of the model formula -- Residuals: ideally, 1Q and 3Q should be very close (i.e., normal distribution)

---

```{r cdfrdddfafvsaffdfasdrs, echo = T, warning=F}
summary(model00)
```
- The coefficients table shows the estimates for intercept and slope

---

```{r cddfdfdsfrdddfafvsaffdfasdrs, echo = T, warning=F}
summary(model00)
```
- RT ~ - 16*proficiency + 1858

---
```{r cdfrddvsabvsffdfasdrs, echo = T, warning=F}
summary(model00)
```
- The model predicts that RTs will be 16 ms faster, given one unit of increase in proficiency.

---

```{r cdfrddvsaffdfaedsdrs, echo = T, warning=F}
summary(model00)
```
- Standard Error (SE) indicates uncertainty around this estimate; larger SE, more uncertainty.

---

```{r cdfrddvsaffddfafafasdrs, echo = T, warning=F}
summary(model00)
```
- t-value is the estimate divided by the standard error: -15.8751/0.7486

---

```{r cdfrddvsassffdfasdrs, echo = T, warning=F}
summary(model00)
```

- P-value indicates whether you can *confidently* reject the null hypothesis that the true value of the estimate is zero (if there is a *significant* effect?)

---

```{r cdssfrddvsaffdfasdrs, echo = T, warning=F}
summary(model00)
```

- R-squared indicates explained variance (higher values show better fit)

---

```{r cdfrddvhsaffdfasdrs, echo = T, warning=F}
summary(model00)
```
- Results of F-test indicate whether you can confidently reject the null hypothesis that the fitted GLM is NOT better fit than a line with slope=0.  

---

```{r cdfrddvsaffdfasfdfdrs, echo = T, warning=F}
summary(model00)
```
- The intercept shows the predicted value of RT at zero proficiency --> ?

---

## Example 1 (continued)
- To help with the interpretation, we center proficiency
- This involves subtracting the mean (of proficiency) from every single proficiency value

---

```{r cdfrddvsaffdfaefasdrs, echo = T, warning=F}
design_lexc$cprof <- design_lexc$prof - mean(design_lexc$prof)
```
- This adds a new column to the data, with each value representing distance from mean proficiency.

```{r cdfrddvsddasfdaffdfasdrs, echo = F, warning=F}
design_lexc %>% slice(1:3)%>% kbl(align = "l") %>% kable_classic(full_width = F, html_font = "Cambria")
```

---

- Let's refit the model (with cprof, NOT with prof)

```{r cdfrddvsaffdfassddrs, echo = T, warning=F}
model01<-lm(RT~cprof, design_lexc)
summary(model01)
```

---
```{r cdfrddsdvsaffdfasdrs, echo = T, eval=FALSE,warning=F}
summary(model01)
```

- Now the intercept shows predicted RT at mean proficiency
- Note that the value for slope (& all other parameters) remain unchanged.

---

## Example 1 (continued)
- We can add more variables into our model.
- Here we add *condition* (word, non-word) to calculate the RT difference between words and nonwords

```{r cdfrddvsasdffdfasdrs, echo = T, warning=F}
model02<-lm(RT~condition+cprof, design_lexc)
```

---
```{r cdfrddvsadfdfafdfasdrs, echo = F, warning=F}
summary(model02)
```
The intercept shows the prediction of the model at the baseline levels of both independent variables, i.e., mean proficiency and *nonword* category of condition (in alphabetical order)

---
```{r cdfrddvsadadfdfafdfasdrs, echo = F, warning=F}
summary(model02)
```
- The model predicts that RTs will be 57ms faster for words than non-words.

---
```{r cdfrddvsadadfdfadffdkfasdrs, echo = F, warning=F}
summary(model02)
```
- Since p-value is <.05, we can *confidently* reject the null hypothesis that the true RT difference between words and non-words is zero, i.e., there is a significant RT difference between words and non-words

---

## The case for random effects

- Importantly, *model01* predicts exactly the same intercept and slopes per participant
- Every participant is expected to have exactly the same RT at mean proficiency for nonwords (474ms), and
- Average RT per participant is expected to be 57 ms faster on words than nonwords. 

---

## The case for random effects (continued)
- The same applies to items, i.e., every item is expected to be associated with the same intercept and slope
- All of our parameters in *model01* are **fixed**, meaning that they do not vary across individuals.
- But is this a reasonable assumption?

---
```{r cdfrwddvsadaddsdfdfadffdkfasdrs, echo = F, warning=F}
model03<-lmer(RT~condition+cprof+(1+condition|subject)+(1+condition|item), design_lexc)
plot_model(model03, type="re", ri.nr=1)+
  geom_hline(yintercept = 0, linetype = "dashed", color = "black")+
  labs(title = "By-subject adjustment", y="Difference from Grand Mean", x="Subject")+
  theme_bw()
```

---
## Random effects of subject
- Clearly, the model estimates do not equally apply to all subjects
- Some participants are faster than others, and some react differently to experimental manipulations
- We need to take this into account, i.e., we should introduce by-subject variability around both the intercept and slope
- This is called random effects, i.e., we expect by-subject random variability around the estimates of the model
- Random effects are categorical variables whose levels are viewed as a sample from some larger population, as opposite to fixed effects, whose levels are of interest in their own right

---

- The models below include both *fixed* and *random* effects, and as such, they are called *mixed effects models*

```{r cdfrwddvsadaddfdfadffdkfasdrs, echo = T,eval=F , warning=F}
lmer(RT~condition+cprof+(1|subject), design_lexc) #by-subject adjustments to the intercept only
lmer(RT~condition+cprof+(0+condition|subject), design_lexc) #by-subject adjustments to the slope only
lmer(RT~condition+cprof+(1+condition|subject), design_lexc) #by-subject adjustments to intercept and slope
```


---

## Random effects of item
- In principle, we could have as many random effects as we want 
- In (Psycho)linguistics and Psychology, it is very common to include both *subjects* and *items* as random effects

---

```{r cdfrwddvsadadsddsdfdfadffdkfasdrs, echo = F, warning=F}
test <-plot_model(model03, type="re", ri.nr=2)+
  geom_hline(yintercept = 0, linetype = "dashed", color = "black")+
  labs(title = "By-item adjustment", y="Difference from Grand Mean", x="Item")+
  theme_bw()

test$data$facet <-ifelse(test$data$facet == "subject (Intercept)", "item (Intercept)",test$data$facet)
test
```

---

## Which random effects to include?
- Some advocate a parsimonious approach, i.e., include random effects only if they improve the model fit
- Some argue that random effects should only be included if they are *theoretically motivated*
- Others argue for a *maximal* random effects structure, i.e., both by-subject and by-item adjustments to the intercept and slope:

```{r cdfrwddvsadadfdweefadffdkfasdrs, echo = T,eval=F , warning=F}
lmer(RT~condition+cprof+(1|subject)+(1|item), design_lexc) #by-subject and by-item adjustments to the intercept only
lmer(RT~condition+cprof+(0+condition|subject)+(0+condition|item), design_lexc) #by-subject and by-item adjustments to the slope only
lmer(RT~condition+cprof+(1+condition|subject)+(1+condition|item), design_lexc) #by-subject and by-item adjustments to the intercept and slope
```

---
The maximal model includes all possible random effects (both by-subject and by-item adjustments to intercept and slopes)

```{r cdfadfdrwddvsadadfdweefadffdkfasdrs, echo = T,eval=F , warning=F}
model03 <- lmer(RT~condition+cprof+(1+condition|subject)+(1+condition|item), design_lexc)
summary(model03)
```

---

```{r cddddfadfdrwddvsadadfdweefaddfffdkfasdrs, echo = F,eval=T , warning=F}
summary(model03)
```

---

- A new *random effects* section is added to the output

```{r cdfdfsadfdrwddvsadadfdweefaddfffdkfasdrs, echo = F,eval=T , warning=F}
summary(model03)$varcor
```

- 126.788 --> average by-subject adjustment to the intercept
-  74.035 --> average by-subject adjustment to the slope
-  -0.873 --> correlation between by-subject adjustments to the intercept and slope. Here this means that faster individuals showed a more pronounced effect to the experimental manipulation.

---

```{r cddffadfdrwddvsadadfdweefaddfffdkfasdrs, echo = F,eval=T , warning=F}
summary(model03)$varcor
```

- 123.784 --> average by-item adjustment to the intercept
-  55.026 --> average by-item adjustment to the slope
-  -0.672 --> correlation between by-item adjustments to the intercept and slope. Here this means that items that on average had faster RTs showed a more pronounced difference between Word and Nonwords.
- 149.083 --> residuals (any other source of variance that is unaccounted for by the model)

---

```{r sddafafahh, echo = F,eval=T , warning=F}
summary(model03)$coefficients
```

- The fixed effects no longer show p-values!
- This is because with random effects, it is no longer possible to derive degrees of freedom analytically
- Solution (1): consider t > 2 as significant 
- Solution (2): conduct model comparisons using the *anova* function
- Solution (3): *approximate* degrees of freedom and calculate p-values. This can be done by adding the *lmerTest* library

---

```{r sdafafahh, echo = T,eval=T , warning=F}
library(lmerTest)
model03 <- lmer(RT~condition+cprof+(1+condition|subject)+(1+condition|item), design_lexc)
summary(model03)$coefficients
```

---

- Let's compare the fixed effects in model02 (no random effects) and model03 (with random effects).

model02 (no random effects):

```{r sdsdafafahh, echo = F,eval=T , warning=F}
summary(model02)$coefficients[-3,] #no random effects
```
 

mode03 (with random effects):
```{r sdsdafsdsafahh, echo = F,eval=T , warning=F}
summary(model03)$coefficients[-3,]
```

- The coefficients are exactly the same, but the model with random effects is less confident (smaller SE) about the effect of *condition* (why?)

---

## Why mixed effects models?

- In repeated measures data, individual observations are NOT independent. This violates the assumption of *independence* in GLMs
- Violating the *independence* assumption may lead to inflated type I error, as the model is overly confident about the estimates, but this is unwarranted!
- The fact that individual rows of data tend to be similar in size does not necessarily mean that any observed difference is due to the experimental manipulation (i.e., significant effect), but may partially suggest that the data are from the same subjects and items
- Mixed models allow for more robust hypothesis testing as they consider both fixed and random effects

---

## Interim summary

- You may include both *continuous* and *categorical* variables in a GLM, but it is usually a good idea to center *continuous* variables. This helps with interpretation but also with computation of model parameters in *mixed effects models*.
- If you have repeated measures, you should include random effects in the model formula to account for random variability around model estimates.

---
## Interim summary

- The choice of random effects structure is debatable, but most will probably agree on a maximal model
- It is also debatable how to derive p-values (& run null hypothesis significance testing) in mixed models
- In this workshop, we approximate degrees of freedom by using the *lmerTest* package, but other approaches are also possible

---
## Example (2)
- Let's consider another example (by Groadner) which examined reading times by L1-English speakers in subject and object relative clauses
- subject relative: the guy who saw the girl
- object relative: the guy who the girl saw

---
## Example (2)

```{r sdsdafsdsasdfahh, echo = T,eval=T, warning=F}
groadner_dat <-read.csv("groadner.csv")
```

```{r sdsdafsddsasdfahh, echo = F,eval=T , warning=F}
groadner_dat %>% slice(1:6)%>% kbl(align = "l") %>% kable_classic(full_width = F, html_font = "Cambria")
groadner_dat$condition <- as.factor(groadner_dat$condition)
```

---
## Example (2)

- We want to create a model to examine the RT difference between object and subject RCs.
- The only fixed effect is *condition*
- We go with a maximal random effects structure:

```{r sdsdafsd, echo = T,eval=T , warning=F}
model04 <- lmer(rawRT ~ condition + (1+condition|subject) + (1+condition|item), groadner_dat)
```

This error indicates that the lmer's underlying machinery did not manage to find a unique solution to estimate the different parameters.

---
## Example (2)

- This is sometimes referred to as *convergence* error. You should ALWAYS check that the model converged successfully before relying on the output
- Convergence issues in lmer are most likely due to random slopes -- the model does not have enough data to estimate random slopes

---
## Example (2)

- What this means is that you are probably being too optimistic in the model formula -- you should simplify the model to achieve convergence
- Given this data, the only way to achieve convergence is to include by-item adjustments to the intercept only (remove all random slopes):

```{r sdssdahh, echo = T,eval=T , warning=F}
model04 <- lmer(rawRT ~ condition + (1|subject) + (1|item), groadner_dat)
```

---

- By default,  *lmer* considers the reference level for categorical variables to be the one that alphabetically occurs prior to the other levels.

```{r sdsfsddsasdfahh, echo = F,eval=T , warning=F}
summary(model04)$coefficient

```

---

```{r sdfsdffdafsddsasdfahh, echo = F,eval=T , warning=F}
plot_model(model04, type = "eff", terms = "condition")+
  theme_bw()+
  geom_hline(yintercept = summary(model04)$coefficient[1,1]+summary(model04)$coefficient[2,1], linetype = "dashed", color = "black")+
  geom_hline(yintercept = summary(model04)$coefficient[1,1], linetype = "dashed", color = "black")+
  geom_text(aes(x = 1.1, y = 495), label = "RT=471 ms", color = "black", size = 5) +
  geom_text(aes(x = 1.9, y = 390), label = "RT=369 ms", color = "black", size = 5)+
  theme(axis.text = element_text(size = 12),
        axis.title = element_text(size = 14))

```


---
## Contrast coding

- Under the hood, R creates a linear combination of our categorical variables (the ones that are inputted as fixed effects) to compare their different levels (subject RC vs. object RC)
- This linear combination is known as *contrast* and the way contrasts are coded is known as *contrast* coding.
- But what is the default contrast?

---
## Example (3): Contrast coding (continued)

```{r sdssddfsdddfafsddsasdfahh, echo = T,eval=T , warning=F}
contrasts(groadner_dat$condition)
```

- The two rows indicate the different levels of condition, and the column shows how these are coded in the *slope* term
- But why does *conditionsubjgap *show the RT difference between subject and object RCs? 

---

## Contrast coding (continued)

- To work with categorical variables, we need to code our contrasts in such a way that our model addresses our **research questions**
- Example (3): Self-paced reading to examine relative clause ambiguities
- *the assistant of the pharmacists who was preparing the medicine* --> DP1 attachment
- *the assistants of the pharmacist who was preparing the medicine* --> DP2 attachment

---

## Contrast coding (continued)

But what about when RC ambiguities are presented in a biasing context? The hypothesis we want to test is that in a DP1-supporting context, RTs will be faster in DP1 attachment, whereas in a DP2-supporting context, RTs will be faster in DP2 attachment (interaction attachment x context)

- attachment (DP1, DP2) x context (DP1, DP2) 

---

```{r sdsdafsdsdfdffasdfahh, echo = T,eval=T, warning=F}
RC_amb <-read.csv("context_amb.csv")
```

```{r sdsdafsddsasdfdfahh, echo = F,eval=T , warning=F}
RC_amb %>% slice(1:6)%>% kbl(align = "l") %>% kable_classic(full_width = F, html_font = "Cambria")
RC_amb$attachment <- as.factor(RC_amb$attachment)
RC_amb$context <- as.factor(RC_amb$context)
```

---

## Contrast coding (continued)
We fit a mixed effects model to examine the interaction between attachment and context. But first, let's examine the default contrasts:

```{r sfffdsdafsdfdfdsasdfahh, echo = T,eval=T , warning=F}
contrasts(RC_amb$attachment)
```

```{r sfffdsfsdfdafsddsasdfahh, echo = T,eval=T , warning=F}
contrasts(RC_amb$context)
```

---

## Contrast coding (continued)
```{r sfsffdsdafsddsadfsdfahh, echo = T,eval=T , warning=F}
model05 <- lmer(RT ~ attachment*context+(1+attachment*context|subject)+ (1+attachment*context|item), RC_amb)
```


---

## Contrast coding (continued)

- We simplify the random effects structure to achieve convergence. 
```{r sfffvfdsdfafsddsasdfahh, echo = T,eval=T , warning=F}
model05 <- lmer(RT ~ attachment*context+(1|subject)+ (1|item), RC_amb) 
```

---

```{r sfffdfsdfdsdafsddsasdfahh, echo = F,eval=T , warning=F}
summary(model05)$coefficients
```

- *Intercept* shows average RT for (DP1-attachment, DP1-context). All other conditions are then compared to this baseline.
- *attachmentDP2 * is the average RT difference between (DP1-attachment, DP1-context) and (DP2-attachment, DP1-context). This is significant -- DP1-supporting context facilitates attachment to DP1.

---

```{r sffdfdfdfffdfdvsdafsddsasdfahh, echo = F,eval=T , warning=F}
summary(model05)$coefficients
```

- *contextDP2* is the average RT difference between (DP1-attachment, DP1-context) and (DP1-attachment, DP2-context)
- *attachmentDP2:contextDP2* is the average RT difference between (DP1-attachment, DP1-context) and (DP2-attachment, DP2-context)

---

## Sum contrasts

- To compute main and interaction effects, we should use sum coding. This involves (1) assigning 1/number_of_levels to each level of categorical variable, and then (2) assigning positive and negative signs to each level to make the desired comparisons:

```{r sdfffdffffffdfdvsdafsddsasdfahh, echo = T,eval=T , warning=F}
contrasts(RC_amb$attachment) <- c(-0.5,0.5)
contrasts(RC_amb$attachment)
```
The slope for *attachment* will show (DP2attachment - DP1attachment)

---

## Sum contrasts

- To compute main and interaction effects, we should use sum coding. With a 2-level categorical variable, this involves assigning (-0.5, 0.5) to the contrast matrix, as below:
```{r sdfffdfffdfsdvsdafsddsasdfahh, echo = T,eval=T , warning=F}
contrasts(RC_amb$context) <- c(-0.5,0.5)
contrasts(RC_amb$context)
```
The slope for *context* will show (DP2context - DP1context)
---
## How to interpret the interaction?

- The interaction term will be: 
- [DP1contxDP1att+ DP2contxDP2att]-[DP1contxDP2att + DP2contxDP1att]
- To simplify, interaction term will show if RTs in DP1- and DP2- context conditions will be any different at DP1- and DP2-attachment.
- Negative values will show facilitation of attachment in a supporting context conditions, while larger values will indicate little attachment facilitation due to context
---

Now we fit the same model again, but this time with sum contrasts:
```{r sfsffffvfdsdfafsddsasdfahh, echo = T,eval=F , warning=F}
model06 <- lmer(RT ~ attachment*context+(1|subject)+ (1|item), RC_amb)
summary(model06)
```

```{r test, echo = F,eval=T , warning=F}
model06 <- lmer(RT ~ attachment*context+(1|subject)+ (1|item), RC_amb)
summary(model06)$coefficients

```

---

- *Intercept* shows average RTs (dashed lines)

```{r fffdsfsfsdsdasdfffvfdsdfafsddsasdfahh, echo = F,eval=T , warning=F}
plot_model(model06, type = "eff", terms = c("context","attachment"))+
  theme_bw()+
  geom_hline(yintercept = summary(model06)$coefficient[1,1], linetype = "dashed", color = "black")+
  theme(axis.text = element_text(size = 12),
        axis.title = element_text(size = 14))

```

---

- Main effect of attachment

```{r fffdsfsffffvfdsdfsdsdaafsddsasdfahh, echo = F,eval=T , warning=F}
plot_model(model06, type = "eff", terms = c("attachment"))+
  theme_bw()+
  theme(axis.text = element_text(size = 12),
        axis.title = element_text(size = 14))
```

---

- Main effect of context

```{r fffdsfsffffvfdsdffdafsddsasdfahh, echo = F,eval=T , warning=F}
plot_model(model06, type = "eff", terms = c("context"))+
  theme_bw()+
  theme(axis.text = element_text(size = 12),
        axis.title = element_text(size = 14))
```

---
## Follow-up on interaction
- The interaction shows that attachment is facilitated in a supporting context. But we still do not know what the preferred attachment strategy is in a DP1-supporting and in a DP2-supporting context.
- To examine this, we fit the same model to (1) items with only a DP1-supporting context, and (2) items with only a DP2-supporting context

---
```{r esdesd, echo = T,eval=T , warning=F}
RC_amb_DP1 <- RC_amb %>% filter(context=="DP1")
```

```{r sfdfsffffvwwwwfdsdfafsddsasdfahh, echo = T,eval=F , warning=F}
model07 <- lmer(RT ~ attachment+(1|subject)+ (1|item), RC_amb_DP1)
summary(model07)
```

```{r tfssdsest, echo = F,eval=T , warning=F}
model07 <- lmer(RT ~ attachment+(1|subject)+ (1|item), RC_amb_DP1)
summary(model07)$coefficients
```
In a DP1-supporting context, DP1 attachment is preferred to DP2 attachment

---
```{r essdsadfddfdesd, echo = T,eval=T , warning=F}
RC_amb_DP2 <- RC_amb %>% filter(context=="DP2")
```

```{r sfdfsffffvfdsdfafsddsasdfahh, echo = T,eval=F , warning=F}
model08 <- lmer(RT ~ attachment+(1|subject)+ (1|item), RC_amb_DP2)
summary(model08)
```

```{r twwwest, echo = F,eval=T , warning=F}
model08 <- lmer(RT ~ attachment+(1|subject)+ (1|item), RC_amb_DP2)
summary(model08)$coefficients
```
In a DP2-supporting context, no significant preference for either DP1 or DP2
---
## Contrast coding (continued)

- A more flexible way around contrast coding is to define your own contrasts. Here are some general guidelines:
- (0) # of contrasts should be decided on based on the number of levels
- (1) contrasts should be independent
- (2) contrasts should be centred -- Weightings should add up to zero
- (3) contrasts should be orthogonal -- no correlation between contrasts
- (4) weightings should allow you to address your research questions
---

```{r fffdhh, echo = F,eval=T , warning=F}
RC_amb$condition <- interaction(RC_amb$context, RC_amb$attachmen)
RC_amb %>% slice(1:3)%>% kbl(align = "l") %>% kable_classic(full_width = F, html_font = "Cambria")
levels(RC_amb$condition)
```

- Three contrasts are needed and the weightings should be either -(1/2) or (1/2) as we have a 2-by-2 design:

---
- Define each contrast using the c() function and bind them using cbind ()
```{r ffdffdhh, echo = T,eval=T , warning=F}
main_contx <- c(1/2,-1/2,1/2,-1/2) #DP1context-DP2context
main_att <- c(1/2,1/2,-1/2,-1/2) #DP1attachment-DP2attachment
int <- c(1/2,-1/2,-1/2,1/2) #(DP1DP1+DP2DP2)-(DP2DP1+DP1DP2)
contrasts(RC_amb$condition)<-cbind(main_contx, main_att, int)
```

```{r ffughuohdffdhh, echo = T,eval=T  , warning=F}
contrasts(RC_amb$condition)
```

---
Fit the model with interaction variable, as it includes all levels of our categorical variables.

```{r ffdffddfdhdfdh, echo = T,eval=T , warning=F}
model07 <- lmer(RT ~ condition+(1|subject)+ (1|item), RC_amb)
```

The output is identical to the output of model06 (which used sum coding)

```{r ffdffdhdfdh, echo = F,eval=T , warning=F}
summary(model07)$coefficients
```

---
## Summary
- Contrasts allow you to examine the effects of categorical variables. Always ask yourself:
- Treatment contrasts compare all levels to a baseline level
- Sum contrasts allow you to derive main and interaction effects (as in a traditional ANOVA analysis)
- You may also create your own contrasts
- You may want to create additional models to explore the source of a significant interaction

---
## Tips for reporting mixed models
Do not forget to discuss:
- how many models you constructed
- what were the fixed and random effects
- how you coded your contrasts
- what packages you used to derive the parameters

---
## Tips for reporting mixed models

When reporting a significant effect, make sure to mention at least the following:
- slope (raw effect size)
- standard error
- t value
- p value (if applicable)
